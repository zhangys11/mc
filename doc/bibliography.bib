@article{bib1,
  title    = {A detailed study on the optical performance of parabolic trough solar collectors with Monte Carlo Ray Tracing method based on theoretical analysis},
  journal  = {Solar Energy},
  volume   = {147},
  pages    = {189-201},
  year     = {2017},
  issn     = {0038-092X},
  doi      = {https://doi.org/10.1016/j.solener.2017.01.055},
  url      = {https://www.sciencedirect.com/science/article/pii/S0038092X17300749},
  author   = {Bin Zou and Jiankai Dong and Yang Yao and Yiqiang Jiang},
  keywords = {Parabolic trough solar collector, Optical performance, Monte Carlo Ray Tracing, Theoretical analysis},
  abstract = {The optical performance of a parabolic trough solar collector (PTC) is studied comprehensively based on Monte Carlo Ray Tracing method (MCRT) and theoretical analysis. The MCRT models are established, and the theoretical equations of several critical parameters are derived firstly. And then the effects of different geometrical parameters on the optical performance are discussed in detail. It is revealed that the distribution of local concentration ratio around the absorber tube changes greatly, and cannot be divided into four parts as previous studies showed for some special parameter conditions. The theoretically derived parameters explain very well the different properties for the critical conditions. The variations of those parameters with different geometrical configurations are further displayed. Accordingly, the optical properties for different critical parameters are discussed. The size relationship between the reflected light cone and the absorber diameter affects the optical efficiency significantly because of the rays escaping effect. Practically, the absorber diameter should be larger than the size of the reflected light cone to avoid great optical loss caused by rays escaping. All the findings in this paper establish the foundation for further research on the optical-to-thermal energy conversion in the PTC system, and provide a reference for designing and optimizing PTC’s structure.}
}

@article{bib2,
  author    = {Arunima Sharma and Srishti Srishti and Vijitha Periyasamy and Manojit Pramanik},
  title     = {{Photoacoustic imaging depth comparison at 532-, 800-, and 1064-nm wavelengths: Monte Carlo simulation and experimental validation}},
  volume    = {24},
  journal   = {Journal of Biomedical Optics},
  number    = {12},
  publisher = {SPIE},
  pages     = {121904},
  keywords  = {photoacoustic imaging, photoacoustic tomography, acoustic resolution photoacoustic microscopy, near-infrared window, imaging depth, Monte Carlo simulation with embedded object, Tissues, Breast, Blood, Monte Carlo methods, Absorbance, Imaging systems, Signal to noise ratio, Photoacoustic imaging, Acquisition tracking and pointing, Optical spheres},
  year      = {2019},
  doi       = {10.1117/1.JBO.24.12.121904},
  url       = {https://doi.org/10.1117/1.JBO.24.12.121904}
}

@article{bib3,
  author   = {Chiriotti, S and Conte, V and Colautti, P and Selva, A and Mairani, A},
  title    = {{MICRODOSIMETRIC SIMULATIONS OF CARBON IONS USING THE MONTE CARLO CODE FLUKA}},
  journal  = {Radiation Protection Dosimetry},
  volume   = {180},
  number   = {1-4},
  pages    = {187-191},
  year     = {2017},
  month    = {09},
  abstract = {{Therapeutic carbon ion beams produce a complex and variable radiation field that changes along the penetration depth due to the high density of energy loss along the particle track together with the secondary particles produced by nuclear fragmentation reactions. An accurate physical characterisation of such complex mixed-radiation fields can be performed by measuring microdosimetric spectra with mini tissue-equivalent proportional counters (mini-TEPCs), which are one of the most accurate devices used in experimental microdosimetry. Numerical calculations with Monte Carlo codes such as FLUKA can be used to supplement experimental microdosimetric measurements performed with TEPCs, but the nuclear cross sections and fragmentation models need to be benchmarked with experimental data for different energies and scenarios. The aim of this work is to compare experimental carbon microdosimetric data measured with the mini TEPC with calculated microdosimetry spectra obtained with FLUKA for 12C ions of 189.5 MeV/u in the Bragg peak region.}},
  issn     = {0144-8420},
  doi      = {10.1093/rpd/ncx201},
  url      = {https://doi.org/10.1093/rpd/ncx201},
  eprint   = {https://academic.oup.com/rpd/article-pdf/180/1-4/187/25409693/ncx201.pdf}
}

@article{bib4,
  author  = {Guifeng, Zhu and Yan, Rui and Honghua, Peng and Ji, Rui-Min and Yu, Shi-He and Liu, Ya-Fen and Tian, Jian and Xu, Bo},
  year    = {2019},
  month   = {02},
  pages   = {},
  title   = {Application of Monte Carlo method to calculate the effective delayed neutron fraction in molten salt reactor},
  volume  = {30},
  journal = {Nuclear Science and Techniques},
  doi     = {10.1007/s41365-019-0557-7}
}

@article{bib5,
  author   = {Teles, P. and Mendes, M. and Zankl, M. and de Sousa, V. and Santos, A. I. and Vaz, P.},
  title    = {{Assessment of the Absorbed Dose in the Kidney of Nuclear Nephrology Paediatric Patients using ICRP Biokinetic Data and Monte Carlo Simulations with Mass-Scaled Paediatric Voxel Phantoms}},
  journal  = {Radiation Protection Dosimetry},
  volume   = {174},
  number   = {1},
  pages    = {121-135},
  year     = {2016},
  month    = {04},
  abstract = {{The aim of this work is to use Monte Carlo simulations and VOXEL phantoms to estimate the absorbed dose in paediatric patients (aged from 2 weeks to 16 y), with normal renal function, to whom technetium-99m-dimercaptosuccinic acid (99mTc-DMSA) was administered, for diagnostic renal scintigraphy purposes; and compare them with values obtained using the International Commission on Radiological Protection (ICRP) methodology. In the ICRP methodology, the cumulated absorbed dose in the kidneys is estimated by multiplying the administered activity with the corresponding given dose coefficients. The other methods were based on Monte Carlo simulations performed on two paediatric voxel phantoms (CHILD and BABY), and another three phantoms, which were modified to suit the mass of the patients’ kidneys, and other anatomical factors. Different S-values were estimated using this methodology, which together with solving the ICRP biokinetic model to determine the cumulated activities, allowed for the estimation of absorbed doses different from those obtained with the ICRP method, together with new dose coefficients. The obtained values were then compared. The deviations suggest that the S-values are strongly dependent on the patient's total body weight, which could be in contrast with the ICRP data, which is provided by age, regardless of other anatomical parameters.}},
  issn     = {0144-8420},
  doi      = {10.1093/rpd/ncw096},
  url      = {https://doi.org/10.1093/rpd/ncw096},
  eprint   = {https://academic.oup.com/rpd/article-pdf/174/1/121/14042240/ncw096.pdf}
}


@article{bib6,
  author  = {Zuo, Yinghong and Zhu, Jin-Hui and Shang, Peng},
  year    = {2021},
  month   = {01},
  pages   = {},
  title   = {Monte Carlo simulation of reflection effects of multi-element materials on gamma rays},
  volume  = {32},
  journal = {Nuclear Science and Techniques},
  doi     = {10.1007/s41365-020-00837-z}
}

@article{bib7,
  author  = {Fazio, Giuseppe and Giaconi, M. and Vetruccio, S.},
  year    = {2018},
  month   = {01},
  pages   = {},
  title   = {A Novel “Monte Carlo” Simulator Aimed to Model High Energy Up-to-date Radiation Sources and Radiation Transport inside Matter},
  volume  = {8},
  journal = {Journal of Physical Science and Application},
  doi     = {10.17265/2159-5348/2018.01.004}
}

@article{bib8,
  author  = {Bedelean, Bogdan},
  year    = {2018},
  month   = {07},
  pages   = {},
  title   = {Application of artificial neural networks and Monte Carlo method for predicting the reliability of RF phytosanitary treatment of wood},
  volume  = {76},
  journal = {European Journal of Wood and Wood Products},
  doi     = {10.1007/s00107-018-1312-1}
}

@article{bib9,
  title    = {Measuring Blood Supply Chain Performance Using Monte-Carlo Simulation},
  journal  = {IFAC-PapersOnLine},
  volume   = {55},
  number   = {10},
  pages    = {2011-2017},
  year     = {2022},
  note     = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2022.10.003},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896322020122},
  author   = {Nirmeen Elsayed and Raghda Taha and Mohamed Hassan},
  keywords = {Blood Supply Chain, Inventory Management, Monte Carlo simulation},
  abstract = {A Monte-Carlo simulation approach is used to model the blood supply chain (BSC) under both supply and demand uncertainty, two different blood supply strategies are generated to analyze the BSC performance. The data used in the validation and verification of the model is a real case study that increases the practical application of the proposed model. The results show that a slight increase in collected blood units will lead to a noticeable increase in minimum and average availability percentage accompanied by an acceptable increase in the total BSC cost, taking into consideration the effect of cost parameters on total cost behavior.}
}

@article{bib10,
  author    = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
  title     = {Monte Carlo Tree Search for Feature Model Analyses: A General Framework for Decision-Making},
  year      = {2021},
  isbn      = {9781450384698},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3461001.3471146},
  doi       = {10.1145/3461001.3471146},
  abstract  = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
  booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
  pages     = {190-201},
  numpages  = {12},
  keywords  = {software product lines, configurable systems, feature models, variability modeling, monte carlo tree search},
  location  = {Leicester, United Kingdom},
  series    = {SPLC '21}
}

@article{bib11,
  author  = {Hwang, Chi-Ok and Kim, Yeongwon and Im, Cheolgi and Lee, Sunggeun},
  year    = {2017},
  month   = {01},
  pages   = {275-279},
  title   = {Buffon's Needle Algorithm to Estimate $\pi$},
  volume  = {08},
  journal = {Applied Mathematics},
  doi     = {10.4236/am.2017.83022}
}

@article{bib12,
  title    = {The cell probe complexity of succinct data structures},
  journal  = {Theoretical Computer Science},
  volume   = {379},
  number   = {3},
  pages    = {405-417},
  year     = {2007},
  note     = {Automata, Languages and Programming},
  issn     = {0304-3975},
  doi      = {https://doi.org/10.1016/j.tcs.2007.02.047},
  url      = {https://www.sciencedirect.com/science/article/pii/S030439750700151X},
  author   = {Anna Gál and Peter Bro Miltersen},
  keywords = {Succinct data structures, Cell probe complexity, Polynomial evaluation with preprocessing},
  abstract = {We consider time-space tradeoffs for static data structure problems in the cell probe model with word size 1 (the bit probe model). In this model, the goal is to represent n-bit data with s=n+r bits such that queries (of a certain type) about the data can be answered by reading at most t bits of the representation. Ideally, we would like to keep both s and t small, but there are tradeoffs between the values of s and t that limit the possibilities of keeping both parameters small. In this paper, we consider the case of succinct representations, where s=n+r for some redundancy r≪n. For a Boolean version of the problem of polynomial evaluation with preprocessing of coefficients, we show a lower bound on the redundancy-query time tradeoff of the form (r+1)t≥Ω(n/logn). In particular, for very small redundancies r, we get an almost optimal lower bound stating that the query algorithm has to inspect almost the entire data structure (up to a logarithmic factor). We show similar lower bounds for problems satisfying a certain combinatorial properties of a coding theoretic flavor, and obtain (r+1)t≥Ω(n) for certain problems. Previously, no ω(m) lower bounds were known on t in the general model for explicit Boolean problems, even for very small redundancies. By restricting our attention to systematic or index structures ϕ satisfying ϕ(x)=x⋅ϕ*(x) for some map ϕ∗ (where ⋅ denotes concatenation), we show similar lower bounds on the redundancy-query time tradeoff for the natural data structuring problems of Prefix Sum and Substring Search.}
}

@article{bib13,
  author  = {Max Warshauer and Eugene Curtin},
  year    = {2006},
  month   = {11},
  pages   = {28-31},
  title   = {The locker puzzle},
  volume  = {28},
  journal = {The Mathematical Intelligencer},
  doi     = {10.1007/BF02986999}
}

@article{bib14,
  title    = {Zipf's Law of Abbreviation and the Principle of Least Effort: Language users optimise a miniature lexicon for efficient communication},
  journal  = {Cognition},
  volume   = {165},
  pages    = {45-52},
  year     = {2017},
  issn     = {0010-0277},
  doi      = {https://doi.org/10.1016/j.cognition.2017.05.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0010027717301166},
  author   = {Jasmeen Kanwal and Kenny Smith and Jennifer Culbertson and Simon Kirby},
  keywords = {Zipf's Law of Abbreviation, Principle of Least Effort, Language universals, Efficient communication, Information theory, Artificial language learning},
  abstract = {The linguist George Kingsley Zipf made a now classic observation about the relationship between a word's length and its frequency; the more frequent a word is, the shorter it tends to be. He claimed that this “Law of Abbreviation” is a universal structural property of language. The Law of Abbreviation has since been documented in a wide range of human languages, and extended to animal communication systems and even computer programming languages. Zipf hypothesised that this universal design feature arises as a result of individuals optimising form-meaning mappings under competing pressures to communicate accurately but also efficiently—his famous Principle of Least Effort. In this study, we use a miniature artificial language learning paradigm to provide direct experimental evidence for this explanatory hypothesis. We show that language users optimise form-meaning mappings only when pressures for accuracy and efficiency both operate during a communicative task, supporting Zipf's conjecture that the Principle of Least Effort can explain this universal feature of word length distributions.}
}

@article{bib15,
  author   = {Andrea Cerioli  and Lucio Barabesi  and Andrea Cerasa  and Mario Menegatti  and Domenico Perrotta },
  title    = {Newcomb-Benford law and the detection of frauds in international trade},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {116},
  number   = {1},
  pages    = {106-115},
  year     = {2019},
  doi      = {10.1073/pnas.1806617115},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1806617115},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806617115},
  abstract = {The contrast of fraud in international trade is a crucial task of modern economic regulations. We develop statistical tools for the detection of frauds in customs declarations that rely on the Newcomb-Benford law for significant digits. Our first contribution is to show the features, in the context of a European Union market, of the traders for which the law should hold in the absence of fraudulent data manipulation. Our results shed light on a relevant and debated question, since no general known theory can exactly predict validity of the law for genuine empirical data. We also provide approximations to the distribution of test statistics when the Newcomb-Benford law does not hold. These approximations open the door to the development of modified goodness-of-fit procedures with wide applicability and good inferential properties.}
}
